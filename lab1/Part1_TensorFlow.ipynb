{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "WBk0ZDWY-ff8"
      },
      "source": [
        "<table align=\"center\">\n",
        "  <td align=\"center\"><a target=\"_blank\" href=\"http://introtodeeplearning.com\">\n",
        "        <img src=\"https://i.ibb.co/Jr88sn2/mit.png\" style=\"padding-bottom:5px;\" />\n",
        "      Visit MIT Deep Learning</a></td>\n",
        "  <td align=\"center\"><a target=\"_blank\" href=\"https://colab.research.google.com/github/moka-ayumu/introtodeeplearning-kr/blob/master/lab1/Part1_TensorFlow.ipynb\">\n",
        "        <img src=\"https://i.ibb.co/2P3SLwK/colab.png\"  style=\"padding-bottom:5px;\" />Run in Google Colab</a></td>\n",
        "  <td align=\"center\"><a target=\"_blank\" href=\"https://github.com/moka-ayumu/introtodeeplearning-kr/blob/main/lab1/Part1_TensorFlow.ipynb\">\n",
        "        <img src=\"https://i.ibb.co/xfJbPmL/github.png\"  height=\"70px\" style=\"padding-bottom:5px;\"  />View Source on GitHub</a></td>\n",
        "</table>\n",
        "\n",
        "\n",
        "# Copyright Information\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3eI6DUic-6jo",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Copyright 2023 MIT Introduction to Deep Learning. All Rights Reserved.\n",
        "# \n",
        "# Licensed under the MIT License. You may not use this file except in compliance\n",
        "# with the License. Use and/or modification of this code outside of MIT Introduction\n",
        "# to Deep Learning must reference:\n",
        "#\n",
        "# © MIT Introduction to Deep Learning\n",
        "# http://introtodeeplearning.com\n",
        "#"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "57knM8jrYZ2t"
      },
      "source": [
        "# Lab 1: TensorFlow 소개 및 RNNs으로 음악 생성\n",
        "\n",
        "이 실습에서는 TensorFlow 사용에 대해 알아보고 딥 러닝 작업을 해결하는 데 TensorFlow를 어떻게 사용할 수 있는지 알아봅니다. 각 셀의 코드를 살펴보고 실행합니다. 그 과정에서 여러 ***TODO*** 블럭을 만나게 됩니다. 해당 셀을 실행하고 계속하기 전에 지침에 따라 블록을 채우십시오.\n",
        "\n",
        "\n",
        "# Part 1: TensorFlow 소개\n",
        "\n",
        "## 0.1 TensorFlow 설치\n",
        "\n",
        "TensorFlow는 기계 학습에 광범위하게 사용되는 소프트웨어 라이브러리입니다. 여기서 계산이 어떻게 표현되는지 TensorFlow에서 간단한 신경망을 정의하는 방법을 배웁니다. Introduction to Deep Learning 2023의 모든 랩에서 최신 버전의 TensorFlow인 TensorFlow 2를 사용할 것입니다. TensorFlow 2는 구문 및 명령형 실행에서 Python과 매우 유사하다는 것을 알 수 있습니다. TensorFlow와 몇 가지 종속성을 설치해 보겠습니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LkaimNJfYZ2w",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "\n",
        "# Download and import the MIT Introduction to Deep Learning package\n",
        "!pip install mitdeeplearning\n",
        "import mitdeeplearning as mdl\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2QNMcdP4m3Vs"
      },
      "source": [
        "## 1.1 왜 TensorFlow는 TensorFlow라 불리는가?\n",
        "\n",
        "TensorFlow는 다차원 배열로 생각할 수 있는 데이터 구조인 Tensor의 흐름(노드/수학 연산)을 처리하기 때문에 'TensorFlow'라고 합니다. Tensor는 문자열이나 정수와 같은 기본 데이터 유형의 n차원 배열로 표현됩니다. Tensor는 벡터와 행렬을 더 높은 차원으로 일반화하는 방법을 제공합니다.\n",
        "\n",
        "Tensor의 ```shape```은 차원의 수와 각 차원의 크기를 정의합니다. Tensor의 ```rank```는 차원(n-dimension)의 수를 제공합니다. 이를 Tensor의 차수 또는 차수로 생각할 수도 있습니다.\n",
        "\n",
        "스칼라가 그 예인 0-d Tensor를 먼저 살펴보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFxztZQInlAB",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "sport = tf.constant(\"Tennis\", tf.string)\n",
        "number = tf.constant(1.41421356237, tf.float64)\n",
        "\n",
        "print(\"`sport` is a {}-d Tensor\".format(tf.rank(sport).numpy()))\n",
        "print(\"`number` is a {}-d Tensor\".format(tf.rank(number).numpy()))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-dljcPUcoJZ6"
      },
      "source": [
        "벡터와 목록을 사용하여 1-d Tensor를 만들 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oaHXABe8oPcO",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "sports = tf.constant([\"Tennis\", \"Basketball\"], tf.string)\n",
        "numbers = tf.constant([3.141592, 1.414213, 2.71821], tf.float64)\n",
        "\n",
        "print(\"`sports` is a {}-d Tensor with shape: {}\".format(tf.rank(sports).numpy(), tf.shape(sports)))\n",
        "print(\"`numbers` is a {}-d Tensor with shape: {}\".format(tf.rank(numbers).numpy(), tf.shape(numbers)))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "gvffwkvtodLP"
      },
      "source": [
        "다음으로 우리는 2-d(즉, 행렬) 및 더 높은 등급의 Tensor 생성을 고려합니다. 예를 들어 이미지 처리 및 컴퓨터 비전과 관련된 향후 랩에서는 4-d Tensor를 사용할 것입니다. 여기서 크기는 배치의 예제 이미지 수, 이미지 높이, 이미지 너비 및 색상 채널 수에 해당합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFeBBe1IouS3",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "### 고차 텐서 정의 ###\n",
        "\n",
        "'''TODO: 2-d Tensor 정의'''\n",
        "matrix = # TODO\n",
        "\n",
        "assert isinstance(matrix, tf.Tensor), \"matrix must be a tf Tensor object\"\n",
        "assert tf.rank(matrix).numpy() == 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zv1fTn_Ya_cz",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "'''TODO: 4-d Tensor 정의'''\n",
        "# tf.zeros를 사용하여 크기가 10 x 256 x 256 x 3인 0으로 되어있는 4-d Tensor를 초기화합니다.\n",
        "#   각 이미지가 RGB 256 x 256인 10개의 이미지로 생각할 수 있습니다.\n",
        "images = # TODO\n",
        "\n",
        "assert isinstance(images, tf.Tensor), \"matrix must be a tf Tensor object\"\n",
        "assert tf.rank(images).numpy() == 4, \"matrix must be of rank 4\"\n",
        "assert tf.shape(images).numpy().tolist() == [10, 256, 256, 3], \"matrix is incorrect shape\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "wkaCDOGapMyl"
      },
      "source": [
        "보시다시피 Tensor의 ```shape```은 각 Tensor 차원의 요소 수를 제공합니다. ```shape```은 매우 유용하며 자주 사용할 것입니다. 슬라이싱을 사용하여 상위 Tensor 내의 하위 텐서에 액세스할 수도 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FhaufyObuLEG",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "row_vector = matrix[1]\n",
        "column_vector = matrix[:,1]\n",
        "scalar = matrix[0, 1]\n",
        "\n",
        "print(\"`row_vector`: {}\".format(row_vector.numpy()))\n",
        "print(\"`column_vector`: {}\".format(column_vector.numpy()))\n",
        "print(\"`scalar`: {}\".format(scalar.numpy()))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "iD3VO-LZYZ2z"
      },
      "source": [
        "## 1.2 Tensors에 대한 계산\n",
        "\n",
        "TensorFlow에서 계산을 생각하고 시각화하는 편리한 방법은 그래프를 사용하는 것입니다. 우리는 이 그래프를 데이터를 보유하고 있는 Tensors와 이러한 Tensors에 어떤 순서로 작용하는 수학적 연산으로 정의할 수 있습니다. 간단한 예를 살펴보고 TensorFlow를 사용하여 이 계산을 정의해 보겠습니다.\n",
        "\n",
        "![alt text](https://raw.githubusercontent.com/moka-ayumu/introtodeeplearning-kr/master/lab1/img/add-graph.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X_YJrZsxYZ2z",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# 그래프에 노드를 만들고 값을 초기화합니다.\n",
        "a = tf.constant(15)\n",
        "b = tf.constant(61)\n",
        "\n",
        "# 값들 추가!\n",
        "c1 = tf.add(a,b)\n",
        "c2 = a + b # TensorFlow는 Tensors에서 작동할 수 있도록 \"+\" 작업을 재정의합니다.\n",
        "print(c1)\n",
        "print(c2)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Mbfv_QOiYZ23"
      },
      "source": [
        "TensorFlow 작업으로 구성된 계산 그래프를 생성한 방법과 값이 76인 Tensors가 출력되는 방법에 주목하세요. 방금 작업으로 구성된 계산 그래프를 생성하고 이를 실행하여 결과를 반환했습니다.\n",
        "\n",
        "이제 약간 더 복잡한 예를 살펴보겠습니다.\n",
        "\n",
        "![alt text](https://raw.githubusercontent.com/moka-ayumu/introtodeeplearning-kr/master/lab1/img/computation-graph.png)\n",
        "\n",
        "여기에서 'a, b'라는 두 개의 입력을 받고 'e' 출력을 계산합니다. 그래프의 각 노드는 입력을 받아 계산을 수행하고 출력을 다른 노드로 전달하는 작업을 나타냅니다.\n",
        "\n",
        "이 계산 함수를 구성하기 위해 TensorFlow에서 간단한 함수를 정의해 보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PJnfzpWyYZ23",
        "scrolled": true,
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "### Tensor 계산들 정의 ###\n",
        "\n",
        "# 간단한 계산 함수 구성\n",
        "def func(a,b):\n",
        "  '''TODO: c, d, e에 대한 연산을 정의합니다(tf.add, tf.subtract, tf.multiply 사용).'''\n",
        "  c = # TODO\n",
        "  d = # TODO\n",
        "  e = # TODO\n",
        "  return e"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "AwrRfDMS2-oy"
      },
      "source": [
        "이제 이 함수를 호출하여 'a,b' 입력이 주어지면 계산 그래프를 실행할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pnwsf8w2uF7p",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# a,b에 대한 예제 값 고려\n",
        "a, b = 1.5, 2.5\n",
        "# 계산 실행\n",
        "e_out = func(a,b)\n",
        "print(e_out)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6HqgUIUhYZ29"
      },
      "source": [
        "출력이 계산 출력에 의해 정의된 값을 가진 Tensor이고 출력이 단일 스칼라 값이므로 모양이 없다는 점에 유의하십시오."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "1h4o9Bb0YZ29"
      },
      "source": [
        "## 1.3 TensorFlow의 신경망(neural networks)\n",
        "TensorFlow에서 신경망을 정의할 수도 있습니다. TensorFlow는 딥러닝 모델을 구축하고 교육하기 위한 강력하고 직관적인 프레임워크를 제공하는 [Keras](https://www.tensorflow.org/guide/keras)라는 고급 API를 사용합니다.\n",
        "\n",
        "먼저 단 하나의 dense layer로 정의된 간단한 퍼셉트론(perceptron)의 예를 살펴보겠습니다. $ y = \\sigma(Wx + b)$, 에서 $W$ 는 가중치 행렬, $b$ 는 bias, $x$ 는 입력, $\\sigma$ 는 시그모이드(sigmoid) 활성 함수, $y$ 는 출력을 나타냅니다. 그래프를 사용하여 이 작업을 시각화할 수도 있습니다.\n",
        "\n",
        "![alt text](https://raw.githubusercontent.com/moka-ayumu/introtodeeplearning-kr/master/lab1/img/computation-graph-2.png)\n",
        "\n",
        "Tensors는 신경망의 구성 요소인 [```Layers```](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer)라는 추상 유형을 통해 흐를 수 있습니다. ```Layers```은 일반적인 신경망 작업을 구현하고 가중치를 업데이트하고 손실을 계산하며 계층 간 연결을 정의하는데 사용됩니다. 위에서 정의한 간단한 퍼셉트론을 구현하기 위해 먼저 ```Layer```을 정의합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HutbJk-1kHPh",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "### Network Layer 정의 ###\n",
        "\n",
        "# n_output_nodes: 출력 노드의 개수\n",
        "# input_shape: 입력의 shape\n",
        "# x: 레이어에 입력되는 값\n",
        "\n",
        "class OurDenseLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, n_output_nodes):\n",
        "    super(OurDenseLayer, self).__init__()\n",
        "    self.n_output_nodes = n_output_nodes\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    d = int(input_shape[-1])\n",
        "    # 매개변수 정의 및 초기화: 가중치 행렬 W 및 편향(bias) b\n",
        "    # 매개변수 초기화는 무작위라는 점에 유의하세요!\n",
        "    self.W = self.add_weight(\"weight\", shape=[d, self.n_output_nodes]) # 차원에 주목\n",
        "    self.b = self.add_weight(\"bias\", shape=[1, self.n_output_nodes]) # 차원에 주목\n",
        "\n",
        "  def call(self, x):\n",
        "    '''TODO: z에 대한 작업 정의(힌트: tf.matmul 사용)'''\n",
        "    z = # TODO\n",
        "\n",
        "    '''TODO: out에 대한 작업 정의(힌트: tf.sigmoid 사용)'''\n",
        "    y = # TODO\n",
        "    return y\n",
        "\n",
        "# layer parameter들은 랜덤으로 초기화되기 때문에 재현성을 위해 랜덤 시드를 설정하겠습니다.\n",
        "tf.random.set_seed(1)\n",
        "layer = OurDenseLayer(3)\n",
        "layer.build((1,2))\n",
        "x_input = tf.constant([[1,2.]], shape=(1,2))\n",
        "y = layer.call(x_input)\n",
        "\n",
        "# 출력 테스트!\n",
        "print(y.numpy())\n",
        "mdl.lab1.test_custom_dense_layer_output(y)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Jt1FgM7qYZ3D"
      },
      "source": [
        "편리하게도 TensorFlow는 신경망에서 일반적으로 사용되는 여러 ```Layers```를 정의해놨습니다. 예를들어, [```Dense```](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense?version=stable)가 있습니다. 이제 단일 ```Layer```를 사용하여 간단한 신경망을 정의하는 대신 Keras의 [`Sequential`](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/Sequential) 모델과 단일 [`Dense`](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/Dense) 레이어를 사용하여 네트워크를 정의합니다. `Sequential` API를 사용하면 빌딩 블록처럼 레이어를 쌓아 신경망을 쉽게 만들 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7WXTpmoL6TDz",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "### Sequential API를 사용하여 신경망(neural network) 정의  ###\n",
        "\n",
        "# 관련 패키지 가져오기\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# 출력할 수 정의\n",
        "n_output_nodes = 3\n",
        "\n",
        "# 먼저 모델을 정의 \n",
        "model = Sequential()\n",
        "\n",
        "'''TODO: z를 계산하기 위해 dense(완전히 연결된) layer을 정의합니다.'''\n",
        "# 기억하세요: dense 레이어는 매개변수 W와 b에 의해 정의됩니다!\n",
        "# TF 문서에서 W 및 b의 초기화에 대해 자세히 알아볼 수 있습니다. :) \n",
        "# https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense?version=stable\n",
        "dense_layer = # TODO\n",
        "\n",
        "# 모델에 dense layer 추가\n",
        "model.add(dense_layer)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "HDGcwYfUyR-U"
      },
      "source": [
        "그게 다야! Sequential API를 사용하여 모델을 정의했습니다. 이제 예제 입력을 사용하여 테스트할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sg23OczByRDb",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# 예제 입력이 있는 테스트 모델\n",
        "x_input = tf.constant([[1,2.]], shape=(1,2))\n",
        "\n",
        "'''TODO: 모델에 입력을 공급하고 출력을 예측하십시오!'''\n",
        "model_output = # TODO\n",
        "print(model_output)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "596NvsOOtr9F"
      },
      "source": [
        "`Sequential` API를 사용하여 모델을 정의하는 것 외에도 모델 훈렴 및 추론을 가능하게 하기 위해 레어를 함께 그룹화하는 [`Model`](https://www.tensorflow.org/api_docs/python/tf/keras/Model?version=stable)클래스를 직접 subclassing하여 신경망을 정의할 수도 있습니다. `Model` 클래스는 우리가 \"모델\" 또는 \"네트워크\"라고 부르는 것을 캡쳐합니다. Subclassing을 사용하여 모델에 대한 클래스를 만든 다음 `call` 함수를 사용하여 네트워크를 통과하는 순방향 패스(forward pass)를 정의할 수 있습니다. Subclassing은 사용자 정의 계층, 사용자 정의 교육 루프, 사용자 정의 활성화 함수, 사용자 정의 모델을 정의할 수 있는 유연성을 제공합니다. 이제 `Sequential` 모델이 아닌 Subclassing을 사용하여 동일한 신경망을 정의해 보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4aCflPVyViD",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "### Subclassing을 사용하여 모델 정의 ###\n",
        "\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "class SubclassModel(tf.keras.Model):\n",
        "\n",
        "  # __init__에서 모델의 layer들을 정의\n",
        "  def __init__(self, n_output_nodes):\n",
        "    super(SubclassModel, self).__init__()\n",
        "    '''TODO: 우리 모델은 단일 Dense 레이어로 구성됩니다. 이 레이어를 정의합니다.''' \n",
        "    self.dense_layer = '''TODO: Dense Layer'''\n",
        "\n",
        "  # call 함수에서 모델의 순방향 패스(forward pass)를 정의합니다.\n",
        "  def call(self, inputs):\n",
        "    return self.dense_layer(inputs)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "U0-lwHDk4irB"
      },
      "source": [
        "`Sequential` API를 사용하여 빌드한 모델과 마찬가지로 예제 입력을 사용하여 `SubclassModel`을 테스트해 보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LhB34RA-4gXb",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "n_output_nodes = 3\n",
        "model = SubclassModel(n_output_nodes)\n",
        "\n",
        "x_input = tf.constant([[1,2.]], shape=(1,2))\n",
        "\n",
        "print(model.call(x_input))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "HTIFMJLAzsyE"
      },
      "source": [
        "중요한 것은 Subclassing이 사용자 정의 모델을 정의할 수 있는 많은 유연성을 제공한다는 것입니다. 예를 들어, `call`함수에서 부울 인수를 사용하여 다양한 네트워크 동작(예. 교육 및 추론 중 다양한 동작)을 지정할 수 있습니다. 일부 인스턴스에서 네트워크가 교란 없이 단순히 입력을 출력하기를 원한다고 가정해 봅시다. 이 동작을 제어하기 위해 부울 인수 `isidentity`를 정의합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P7jzGX5D1xT5",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "### Subclassing을 사용하여 모델 정의 및 사용자 지정 동작 ###\n",
        "\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "class IdentityModel(tf.keras.Model):\n",
        "\n",
        "  # 이전과 마찬가지로 __init__에서 모델의 레이러를 정의합니다.\n",
        "  # 우리가 원하는 동작은 순방향 패스(forward pass)를 포함하므로 이 부분은 변경되지 않습니다.\n",
        "  def __init__(self, n_output_nodes):\n",
        "    super(IdentityModel, self).__init__()\n",
        "    self.dense_layer = tf.keras.layers.Dense(n_output_nodes, activation='sigmoid')\n",
        "\n",
        "  '''TODO: 네트워크가 isidentity 인수의 제어에 따라 변경되지 않은 입력을 출력하는 동작을 구현합니다'''\n",
        "  def call(self, inputs, isidentity=False):\n",
        "    x = self.dense_layer(inputs)\n",
        "    '''TODO: identity 동작 구현'''"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Ku4rcCGx5T3y"
      },
      "source": [
        "이 동작을 테스트해 보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NzC0mgbk5dp2",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "n_output_nodes = 3\n",
        "model = IdentityModel(n_output_nodes)\n",
        "\n",
        "x_input = tf.constant([[1,2.]], shape=(1,2))\n",
        "'''TODO: 입력을 모델에 전달하고 입력 identity 옵션을 사용하거나 사용하지 않고 호출합니다.'''\n",
        "out_activate = # TODO\n",
        "out_identity = # TODO\n",
        "\n",
        "print(\"Network output with activation: {}; network identity output: {}\".format(out_activate.numpy(), out_identity.numpy()))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "7V1dEqdk6VI5"
      },
      "source": [
        "이제 우리는 `Sequential` 및 Subclassing API를 모두 사용하여 TensorFlow에서 `Layers`와 신경망을 정의하는 방법을 배웠으므로 역전파(backpropagation)를 통해 실제로 네트워크 교육을 구현하는 방법에 관심을 돌릴 준비가 되었습니다."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "dQwDhKn8kbO2"
      },
      "source": [
        "## 1.4 TensorFlow의 Automatic differentiation\n",
        "\n",
        "[Automatic differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation)\n",
        "는 TensorFlow의 가장 중요한 부분 중 하나이며 \n",
        "[backpropagation](https://en.wikipedia.org/wiki/Backpropagation)\n",
        "를 통한 교육의 중추입니다. TensorFlow GradientTape [`tf.GradientTape`](https://www.tensorflow.org/api_docs/python/tf/GradientTape?version=stable)를 사용하여 나중에 그라디언트를 계산하기 위한 작업을 추적합니다. \n",
        "\n",
        "순방향 패스(forward pass)가 네트워크를 통해 이루어지면 모든 순방향 패스 작업이 \"테이프(tape)\"에 기록됩니다. 그런 다음 그래디언트를 계산하기 위해 테이프를 거꾸로 재생합니다. 기본적으로 테이프는 뒤로 재생한 후 폐기됩니다. 즉, 특정 `tf.GradientTape`는 하나의 그래디언트만 계산할 수 있으며 후속 호출에서 런타임 오류가 발생합니다. 그러나 ```persistent``` 그래디언트 테이프를 생성하여 동일한 계산에 대해 여러 그래디언트를 계산할 수 있습니다.\n",
        "\n",
        "먼저 GradientTape를 사용하여 그래디언트를 계산하고 계산을 위해 액세스하는 방법을 살펴보겠습니다. 간단한 함수 $ y = x^2$ 를 정의하고 그래디언트를 계산합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tdkqk8pw5yJM",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "### GradientTape로 Gradient computation ###\n",
        "\n",
        "# y = x^2\n",
        "# 예: x = 3.0\n",
        "x = tf.Variable(3.0)\n",
        "\n",
        "# Gradient tape 시작\n",
        "with tf.GradientTape() as tape:\n",
        "  # 함수 정의\n",
        "  y = x * x\n",
        "# Gradient 접근 -- x에 대한 y의 도함수\n",
        "dy_dx = tape.gradient(y, x)\n",
        "\n",
        "assert dy_dx.numpy() == 6.0"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "JhU5metS5xF3"
      },
      "source": [
        "신경망 훈련에서 우리는 미분 및 확률적 경사하강법(stochastic gradient descent, SGD)을 사용하여 손실 함수를 최적화합니다. 이제 `GradientTape`를 사용하여 미분을 계산하고 액세스하는 방법을 이해했으므로 자동 미분 및 SGD를 사용하여 $L=(x-x_f)^2$ 의 최소값을 찾는 예를 살펴보겠습니다. 여기서 $x_f$ 는 최적화하려는 원하는 값에 대한 변수입니다. $L$ 는 우리가 최소화하려는 손실을 나타냅니다. 이 문제를 분석적으로 ($x_{min}=x_f$) 명확하게 해결할 수 있지만 `GradientTape`를 사용하여 이 문제를 계산하는 방법을 고려하면 전체 신경망 손실을 최적화하기 위해 경사 하강법을 사용하는 미래의 랩에 적합합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "attributes": {
          "classes": [
            "py"
          ],
          "id": ""
        },
        "id": "7g1yWiSXqEf-",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "### 자동 미분 및 SGD를 통한 함수 최소화 ###\n",
        "\n",
        "# initial에 대한 임의의 값을 초기화 x\n",
        "x = tf.Variable([tf.random.normal([1])])\n",
        "print(\"Initializing x={}\".format(x.numpy()))\n",
        "\n",
        "learning_rate = 1e-2 # SGD의 학습률\n",
        "history = []\n",
        "# 목표값 정의\n",
        "x_f = 4\n",
        "\n",
        "# SGD를 여러 번 반복 실행합니다. 각 반복에서 우리는 손실을 계산하고, \n",
        "#   x에 대한 손실의 도함수를 계산하고, SGD 업데이트를 수행합니다.\n",
        "for i in range(500):\n",
        "  with tf.GradientTape() as tape:\n",
        "    '''TODO: 위에서 설명한 대로 손실을 정의'''\n",
        "    loss = # TODO\n",
        "\n",
        "  # gradient tape를 이용한 손실 최소화\n",
        "  grad = tape.gradient(loss, x) # x에 대한 손실의 도함수를 계산\n",
        "  new_x = x - learning_rate*grad # sgd 업데이트\n",
        "  x.assign(new_x) # x값 업데이트\n",
        "  history.append(x.numpy()[0])\n",
        "\n",
        "# x_f를 향해 최적화하면서 x의 진화를 플로팅!\n",
        "plt.plot(history)\n",
        "plt.plot([0, 500],[x_f,x_f])\n",
        "plt.legend(('Predicted', 'True'))\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('x value')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "pC7czCwk3ceH"
      },
      "source": [
        "`GradientTape`는 자동 차별화를 위한 매우 유연한 프레임워크를 제공합니다. 신경망을 통해 오류를 역전파하기 위해 테이프에서 순방향 패스를 추적하고 이 정보를 사용하여 기울기를 결정한 다음 이러한 기울기를 SGD를 사용하여 최적화에 사용합니다."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "WBk0ZDWY-ff8"
      ],
      "name": "Part1_TensorFlow.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
